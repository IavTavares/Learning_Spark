{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/15 18:45:05 WARN Utils: Your hostname, ivo-Nitro-AN515-57 resolves to a loopback address: 127.0.1.1; using 192.168.1.197 instead (on interface wlp0s20f3)\n",
      "22/07/15 18:45:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/15 18:45:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/07/15 18:45:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    ".builder\n",
    ".appName(\"Spark_ML_APP\")\n",
    ".enableHiveSupport() # we need this to create tables\n",
    ".getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "22/07/15 18:45:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "\n",
      "\n",
      "There are 5780 rows in the training set,\n",
      "and 1366 in the test set\n"
     ]
    }
   ],
   "source": [
    "filePath = \"\"\"../data/sf-airbnb-clean.parquet/\"\"\"\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\n",
    "\"number_of_reviews\", \"price\").show(5)\n",
    "\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "print(f\"\"\"\\n\\nThere are {trainDF.count()} rows in the training set,\n",
    "and {testDF.count()} in the test set\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should cache the trainDF dataframe, since we'll be using it often\n",
    "# this is important, when in the presence of a distributed system\n",
    "# I'm just using my localhost as server for the data, instead...\n",
    "\n",
    "# trainDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression model predicting price given the number of bedrooms, \"bathrooms\", and \"number_of_reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------+---------------+-----+\n",
      "|bedrooms|bathrooms|number_of_reviews|       features|price|\n",
      "+--------+---------+-----------------+---------------+-----+\n",
      "|     1.0|      1.0|              1.0|  [1.0,1.0,1.0]|200.0|\n",
      "|     1.0|      1.0|             13.0| [1.0,1.0,13.0]|130.0|\n",
      "|     1.0|      1.0|             12.0| [1.0,1.0,12.0]| 95.0|\n",
      "|     1.0|      1.0|              1.0|  [1.0,1.0,1.0]|250.0|\n",
      "|     3.0|      3.0|              0.0|  [3.0,3.0,0.0]|250.0|\n",
      "|     1.0|      1.0|            100.0|[1.0,1.0,100.0]|115.0|\n",
      "|     1.0|      1.5|             36.0| [1.0,1.5,36.0]|105.0|\n",
      "|     1.0|      1.0|            194.0|[1.0,1.0,194.0]| 86.0|\n",
      "|     1.0|      1.0|              4.0|  [1.0,1.0,4.0]|100.0|\n",
      "|     2.0|      1.0|              2.0|  [2.0,1.0,2.0]|220.0|\n",
      "+--------+---------+-----------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"bedrooms\",\"bathrooms\", \n",
    "                                \"number_of_reviews\"], outputCol=\"features\")\n",
    "# to do ML modeling like a Linear regression, we need to put our data into\n",
    "# a single vector\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select(\"bedrooms\",\"bathrooms\", \"number_of_reviews\", \"features\", \"price\").show(10)\n",
    "#  column should be 'equal' to the features column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/15 18:45:09 WARN Instrumentation: [7564bcb2] regParam is zero, which might cause numerical instability and overfitting.\n",
      "22/07/15 18:45:09 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "22/07/15 18:45:09 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/07/15 18:45:09 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/07/15 18:45:09 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is\n",
      "price = 116.65*bedrooms + 14.65 * bathrooms + -0.25 * bathrooms + 48.22\n"
     ]
    }
   ],
   "source": [
    "m_0 = round(lrModel.coefficients[0], 2)\n",
    "m_1 = round(lrModel.coefficients[1], 2)\n",
    "m_2 = round(lrModel.coefficients[2], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "print(f\"\"\"The formula for the linear regression line is\n",
    "price = {m_0}*bedrooms + {m_1} * bathrooms + {m_2} * bathrooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([116.6457, 14.6487, -0.2452])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.6583813605265805,\n",
       " 5.377873367940104,\n",
       " 0.055272275442847144,\n",
       " 8.974083827855292]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.summary.coefficientStandardErrors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pipeline in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/15 18:45:10 WARN Instrumentation: [2437f49f] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------+--------+---------------+------+------------------+\n",
      "|bedrooms|bathrooms|number_of_reviews|bedrooms|       features| price|        prediction|\n",
      "+--------+---------+-----------------+--------+---------------+------+------------------+\n",
      "|     1.0|      1.0|            128.0|     1.0|[1.0,1.0,128.0]|  85.0|148.12966631945827|\n",
      "|     1.0|      1.0|              0.0|     1.0|  [1.0,1.0,0.0]|  45.0|179.51913339774154|\n",
      "|     1.0|      1.0|              0.0|     1.0|  [1.0,1.0,0.0]|  70.0|179.51913339774154|\n",
      "|     1.0|      1.0|              1.0|     1.0|  [1.0,1.0,1.0]| 128.0|179.27390318619246|\n",
      "|     1.0|      1.0|              3.0|     1.0|  [1.0,1.0,3.0]| 159.0|178.78344276309429|\n",
      "|     2.0|      1.0|             15.0|     2.0| [2.0,1.0,15.0]| 250.0| 292.4864106206108|\n",
      "|     1.0|      1.0|              1.0|     1.0|  [1.0,1.0,1.0]|  99.0|179.27390318619246|\n",
      "|     1.0|      1.0|              0.0|     1.0|  [1.0,1.0,0.0]|  95.0|179.51913339774154|\n",
      "|     1.0|      1.0|              0.0|     1.0|  [1.0,1.0,0.0]| 100.0|179.51913339774154|\n",
      "|     1.0|      1.0|              0.0|     1.0|  [1.0,1.0,0.0]|2010.0|179.51913339774154|\n",
      "+--------+---------+-----------------+--------+---------------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"bedrooms\",\"bathrooms\", \"number_of_reviews\",\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost',\n",
       " 'cancellation_policy',\n",
       " 'instant_bookable',\n",
       " 'neighbourhood_cleansed',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'bed_type']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes\n",
    "                    if dataType == \"string\"]\n",
    "categoricalCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhostOHE',\n",
       " 'cancellation_policyOHE',\n",
       " 'instant_bookableOHE',\n",
       " 'neighbourhood_cleansedOHE',\n",
       " 'property_typeOHE',\n",
       " 'room_typeOHE',\n",
       " 'bed_typeOHE',\n",
       " 'host_total_listings_count',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'bedrooms_na',\n",
       " 'bathrooms_na',\n",
       " 'beds_na',\n",
       " 'review_scores_rating_na',\n",
       " 'review_scores_accuracy_na',\n",
       " 'review_scores_cleanliness_na',\n",
       " 'review_scores_checkin_na',\n",
       " 'review_scores_communication_na',\n",
       " 'review_scores_location_na',\n",
       " 'review_scores_value_na']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "# A common approach is to use the StringIndexer and OneHotEncoder. With this approach, the first step is to\n",
    "# apply the StringIndexer estimator to convert categorical values into category indices. These category indices \n",
    "# are ordered by label frequencies, so the most frequent label gets index 0, which provides us with reproducible \n",
    "# results across various runs of the same data. Once you have created your category indices, you can pass those\n",
    "# as input to the OneHotEncoder.\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "                                outputCols=indexOutputCols,\n",
    "                                handleInvalid=\"keep\")\n",
    "# handleInvalid parameter specifies how to handle categories in test set, but not in the train set. The options\n",
    "# are skip (filter out rows with invalid data), error (throw an error), or keep (put inva‚Äê\n",
    "# lid data in a special additional bucket, at index numLabels                               \n",
    "\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "                            outputCols=oheOutputCols)\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes\n",
    "                if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "assemblerInputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs,\n",
    "                                outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/15 18:46:45 WARN Instrumentation: [3bbd1622] regParam is zero, which might cause numerical instability and overfitting.\n",
      "22/07/15 18:46:45 WARN Instrumentation: [3bbd1622] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "+--------------------+------+------------------+\n",
      "|            features| price|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|(105,[0,4,8,25,47...|  85.0| 54.97283005654026|\n",
      "|(105,[0,4,8,25,47...|  45.0| 23.34747155520563|\n",
      "|(105,[0,4,8,25,47...|  70.0| 28.46158915392334|\n",
      "|(105,[0,4,8,15,46...| 128.0|-91.65206355943246|\n",
      "|(105,[0,4,8,15,47...| 159.0| 95.14003645319463|\n",
      "|(105,[0,4,8,15,47...| 250.0|263.66204150177145|\n",
      "|(105,[0,4,8,14,46...|  99.0| 152.5327799254228|\n",
      "|(105,[0,4,8,34,46...|  95.0|180.84328858035042|\n",
      "|(105,[0,4,8,31,46...| 100.0|-52.85953832967152|\n",
      "|(105,[0,4,8,31,47...|2010.0| 261.2786481467865|\n",
      "+--------------------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                            labelCol=\"price\",\n",
    "                                            metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 is 0.16055708814862335\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R^2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath = \"../data/lr-pipeline-model\"\n",
    "pipelineModel.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.spark_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cad669ada231548fc6a6bcdb5e46bad24e3100816ded594559296e7eb00606c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
